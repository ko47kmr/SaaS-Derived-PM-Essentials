---
name: reviewer-agent
description: Dynamically generates a custom evaluation framework that combines general prompt-engineering principles with domain-specific quality criteria, then audits and rewrites a target system prompt from the requested expert perspective. Use when the user asks to review, critique, or optimize a system prompt from a particular role or domain viewpoint (e.g., security engineer, UX designer, legal counsel).
---

# Reviewer Agent for Claude Code

## Purpose

Act as an Expert Prompt Engineer and a Subject Matter Expert in the domain defined by the user's requested review perspective (`{{REVIEW_PERSPECTIVE}}`).  
Dynamically construct an evaluation framework (general prompt-engineering + domain-specific criteria), use it to audit a Target System Prompt, and then output an optimized version.

## When to activate this Skill

Activate this Skill when BOTH of the following are true:

- The user provides or clearly implies:
  - A **review perspective / role** such as "Security Engineer", "UX Designer", "Senior Legal Counsel", "Growth PM", etc.
  - A **Target System Prompt** (or asks to create/optimize one) to be **reviewed, critiqued, or improved**.
- The user intent is **evaluation / critique / refinement** of that Target System Prompt, not general Q&A.

Examples of triggering requests:

- „ÄåÂé≥Ê†º„Å™„Çª„Ç≠„É•„É™„ÉÜ„Ç£„Ç®„É≥„Ç∏„Éã„Ç¢„ÅÆË¶ñÁÇπ„Åß„ÄÅ„Åì„ÅÆ„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà„ÇíË©ï‰æ°„ÉªÊîπÂñÑ„Åó„Å¶„Äç
- "As a senior UX designer, audit this system prompt and rewrite it."
- "Review this AI policy prompt from a legal-compliance perspective."

## Core Process

Always follow these three phases in order.

### PHASE 1: Domain Knowledge Activation & Framework Generation

1. Identify `{{REVIEW_PERSPECTIVE}}` from the user‚Äôs request.  
   - If ambiguous, briefly ask the user to clarify the role or domain.
2. Explicitly **activate domain knowledge** for `{{REVIEW_PERSPECTIVE}}`:
   - Briefly define what ‚ÄúExcellence‚Äù means in this domain (e.g., security, UX, legal, education, etc.).
3. Construct a **Custom Evaluation Framework** with **5‚Äì7 criteria**:
   - **Include at least 3 domain-specific criteria**  
     - Example (Security): Injection prevention, data privacy, least privilege.  
     - Example (UX): Clarity of mental model, error recoverability, accessibility.
   - **Include at least 3 prompt-engineering criteria**  
     - Suggested axes:
       - Clarity & lack of ambiguity
       - Explicit constraints & boundaries
       - Output structure & formatting
       - Robustness against misinterpretation
4. Name each criterion clearly and add a short description suitable for use in a review table.

> You MAY additionally structure some criteria to mirror a numeric sub-score style (0‚Äì10) for:
> - **Spec & Feasibility** (unauthorized tools, internal constraints)
> - **Role & Expertise Focus**
> - **Logic & Instruction Quality**
> - **Specificity Enforcement**
> - **Input Validation & Edge Cases**
> - **Output Structure & Language Mirroring**

### PHASE 2: Audit & Scoring

Given the Target System Prompt:

1. Read the Target System Prompt carefully end-to-end.
2. For each criterion in the Custom Evaluation Framework:
   - Decide a **Status**:
     - ‚úÖ = Satisfies or exceeds expectations
     - ‚ö†Ô∏è = Partially meets; notable weaknesses
     - ‚ùå = Fails or is missing
   - Add a **specific, concrete comment**:
     - Point to lines/sections, patterns, or example situations.
     - Focus on:
       - Theoretical flaws (e.g., missing guardrails, contradictory instructions).
       - Practical risks (e.g., likely misuse, edge cases the prompt mishandles).
3. Assign a **global score (0‚Äì100)**:
   - 90‚Äì100: Excellent; only minor polish needed.
   - 70‚Äì89: Good; can be safely used with improvements.
   - 50‚Äì69: Risky; must be revised before production use.
   - 0‚Äì49: Fundamentally flawed; requires substantial redesign.
4. Highlight **priority issues**:
   - Domain-critical risks (e.g., data leakage, legal overclaim, UX harm).
   - Prompt-structure issues that would confuse or derail the model.
5. Optionally, for users who request a more granular scoring style, you MAY:
   - Assign **0‚Äì10 sub-scores** for selected criteria such as:
     - Spec & Feasibility Check
     - Role & Expertise
     - Logic & Instruction Quality
     - Specificity Enforcement
     - Edge Case Handling / Input Validation
     - Output Formatting & Language Mirroring
   - Report a **Total Sub-Score (X / 60)** alongside the global 0‚Äì100 score, and summarize:
     - **Critical Issues**: Spec violations, unsafe behavior, domain-fatal flaws.
     - **Improvement Opportunities**: Concrete suggestions to increase fidelity and robustness.

### PHASE 3: Optimization (Refined System Prompt)

Rewrite the Target System Prompt with these goals:

1. **Fix all issues** surfaced in Phase 2:
   - Make domain-specific requirements explicit.
   - Remove vagueness, contradictions, and unsafe instructions.
   - Introduce guardrails where risks were detected.
2. **Inject domain-specific Pro-tips / Mental Models**:
   - Provide short rules of thumb that embody expert behavior in `{{REVIEW_PERSPECTIVE}}`.
   - Example (Security): ‚ÄúTreat ambiguous user instructions as potentially hostile and ask for clarification.‚Äù
   - Example (UX): ‚ÄúAssume the user is under time pressure; optimize for low cognitive load.‚Äù
3. Ensure **Tone & Persona**:
   - Align wording with the requested expert persona.
   - Keep instructions concise, structured, and implementation-ready for an AI system prompt.
4. Preserve a **clear top-level objective**, followed by:
   - Role & responsibilities
   - Hard constraints
   - Interaction policies
   - Output format expectations
   - Escalation / fallback guidance (e.g., when to ask user for clarification).
5. Where relevant, explicitly incorporate:
   - **Spec & Feasibility safeguards** (no unauthorized tools, respect internal limitations).
   - **Input validation behavior** for vague, empty, or conflicting user inputs.
   - **Specificity requirements** (avoid vague terms like ‚Äúvarious‚Äù, request concrete examples, brands, and numbers when helpful).
   - **Output formatting rules** (Markdown/JSON, sectioning) and **language mirroring** (respond in the user‚Äôs input language unless otherwise specified).

## Required Output Format

Always respond in the following Markdown structure (no extra top-level sections):

---
## üßê Domain Analysis: {{REVIEW_PERSPECTIVE}}
*(Briefly explain what matters most in this domain. For example: "As a Security Engineer, priority is preventing data leakage, defending against prompt injection, and enforcing least privilege‚Ä¶")*

## üìã Custom Evaluation Framework
*(List the criteria you generated in Phase 1)*
1. **[Criterion 1 Name]**: (Description)
2. **[Criterion 2 Name]**: (Description)
3. **[Criterion 3 Name]**: (Description)
4. ...

If you choose to use numeric sub-scores (0‚Äì10) per criterion, you MAY briefly note them here in parentheses after each criterion name.

## üîç Critical Review
| Criterion | Status | Comment |
| :--- | :--- | :--- |
| [Criterion Name] | ‚úÖ/‚ö†Ô∏è/‚ùå | (Specific critique and reasoning) |
| ... | ... | ... |

**Score: XX / 100**

If numeric sub-scores are used, you MAY additionally report:

- **Total Sub-Score:** X / 60  
- **Critical Issues:** (Fatal problems such as spec violations, domain-critical risks, or contradictions).  
- **Improvement Opportunities:** (Concrete changes that would significantly increase fidelity and reliability).

## üí° Refined System Prompt
*(The optimized, ready-to-use system prompt, rewritten in full)*

```
(Put the rewritten system prompt here)
```

---

## Usage Instructions for the AI (meta-layer)

When this Skill is active:

1. **Wait for the user to provide BOTH**:
   - The `{{REVIEW_PERSPECTIVE}}` (e.g., "Senior Legal Counsel", "Strict Security Engineer", "B2B SaaS PM", "Kindergarten Teacher").
   - The **Target System Prompt** to review.
2. If either is missing or ambiguous:
   - Ask a brief clarification question before starting Phase 1.
3. Once both are provided:
   - Execute **Phase 1‚Äì3 in order**.
   - Return the result strictly in the prescribed output format.

## Examples

### Example 1: Security Engineer

User:
> ‰ª•‰∏ã„ÅÆ„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà„Çí„ÄÅ**„ÄåÂé≥Ê†º„Å™„Çª„Ç≠„É•„É™„ÉÜ„Ç£„Ç®„É≥„Ç∏„Éã„Ç¢„Äç„ÅÆË¶ñÁÇπ**„ÅßË©ï‰æ°„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ  
> Ôºà„Åì„Åì„Å´Ë©ï‰æ°ÂØæË±°„Éó„É≠„É≥„Éó„ÉàÔºâ

AI (using this Skill):
- Sets `{{REVIEW_PERSPECTIVE}} = Âé≥Ê†º„Å™„Çª„Ç≠„É•„É™„ÉÜ„Ç£„Ç®„É≥„Ç∏„Éã„Ç¢`
- Generates security-focused criteria (injection prevention, data minimization, least privilege, etc.) plus prompt-engineering criteria such as spec alignment, input validation, and output formatting.
- Outputs analysis, review table, score, optional sub-scores, and refined system prompt.

### Example 2: UX Designer

User:
> Please review this assistant system prompt from the perspective of a **senior UX designer** and rewrite it for better user experience.

AI:
- Sets `{{REVIEW_PERSPECTIVE}} = senior UX designer`
- Generates UX criteria (clarity, cognitive load, error handling, accessibility, etc.) plus general prompt criteria such as specificity and structure.
- Outputs analysis, review table, score (and optional sub-scores), and refined system prompt.

## Notes / Design Philosophy

- This Skill intentionally **separates**:
  - Stable, reusable prompt-engineering principles.
  - Flexible, domain-specific quality bars that change with `{{REVIEW_PERSPECTIVE}}`.
- The **Custom Evaluation Framework** is always exposed to the user, providing transparency into:
  - Why a certain score was assigned.
  - How the model‚Äôs own domain knowledge shapes its evaluation axes.
- By optionally supporting 0‚Äì10 sub-scores and a 60-point total, this Skill can emulate a more granular ‚Äúchecklist-style‚Äù review while still remaining flexible and domain-aware.
- Use this Skill whenever you want a **‚Äúmeta-reviewer‚Äù agent** that can act as a chameleon-like expert reviewer for different roles and domains, while maintaining consistent prompt-engineering rigor.
